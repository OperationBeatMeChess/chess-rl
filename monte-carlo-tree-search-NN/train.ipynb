{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, pickle\n",
    "from tqdm import tqdm  # notebook compatible\n",
    "import wandb\n",
    "import numpy as np\n",
    "import gym\n",
    "import chess\n",
    "\n",
    "import adversarial_gym\n",
    "from OBM_ChessNetwork import Chess42069NetworkSimple\n",
    "from search import MonteCarloTreeSearch\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Gym Chess Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Chess-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/.local/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Chess42069NetworkSimple(\n",
       "  (swin_transformer): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): Sequential(\n",
       "      (0): SwinTransformerStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.004)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.013)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.017)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.022)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.026)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.030)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.035)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.039)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.043)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.048)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.052)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.061)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.065)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.070)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.074)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.078)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.083)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.087)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.096)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): ClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (flatten): Identity()\n",
       "    )\n",
       "  )\n",
       "  (policy_head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
       "    (2): Linear(in_features=512, out_features=4672, bias=True)\n",
       "  )\n",
       "  (value_head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (value_loss): MSELoss()\n",
       "  (policy_loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = '/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/best_baseSwinChessNet.pt'\n",
    "BESTMODEL_SAVEPATH = 'mcts_baseSwinChessNet_best.pt'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = Chess42069NetworkSimple(hidden_dim=512, device=DEVICE)\n",
    "best_model = Chess42069NetworkSimple(hidden_dim=512, device=DEVICE)\n",
    "\n",
    "if MODEL_PATH is not None:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    best_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MCTS Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = MonteCarloTreeSearch(env, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReplayBuffer:\n",
    "#     \"\"\" Replay buffer to store past experiences for training policy/value network\"\"\"\n",
    "#     def __init__(self, capacity):\n",
    "#         self.actions = np.empty((capacity, 1), dtype=int)\n",
    "#         self.states = np.empty((capacity, 8, 8), dtype=int)\n",
    "#         self.values = np.empty((capacity, 1), dtype=float)\n",
    "\n",
    "#         self.buffer = []\n",
    "#         self.capacity = capacity\n",
    "#         self.position = 0\n",
    "    \n",
    "#     def push(self, state, action, value):    \n",
    "#         if len(self.buffer) < self.capacity:\n",
    "#             self.buffer.append(None)\n",
    "#         self.buffer[self.position] = (state, action, value)\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def update(self, states, actions, values):\n",
    "#         for state, action, value in zip(states, actions, values):\n",
    "#             self.push(state, action, value)\n",
    "\n",
    "#     def sample(num_samples):\n",
    "#         pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, white, black, perspective=None, num_sims=1000):\n",
    "    \"\"\" \n",
    "    Plays a game and returns 1 if chosen perspective has won, else 0.\n",
    "    \n",
    "    Perspective is either Chess.WHITE (1) or Chess.BLACK (0).\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        state = env.get_string_representation()\n",
    "        if step % 2 == 0:\n",
    "            action, ucb = white.search(state, obs, simulations_number=num_sims)\n",
    "        else:\n",
    "            action, ucb = black.search(state, obs, simulations_number=num_sims)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "\n",
    "    # return reward based on winning or losing from white/black perspective\n",
    "    if perspective == chess.BLACK and reward == -1:\n",
    "        reward = 1\n",
    "    elif perspective == chess.WHITE and reward == 1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def play_game(env, white, black, perspective: int = None, sample_n: int =1):\n",
    "    \"\"\" \n",
    "    Play a game and returns whether white or black white. \n",
    "    \n",
    "    Perspective - Chess.WHITE (1) or Chess.BLACK (0).\n",
    "    sample_n - Set number of top moves to sample from\n",
    "\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        if step % 2 == 0:\n",
    "            action, log_prob = white.get_action(obs[0], env.board.legal_moves, sample_n=sample_n)\n",
    "        else:\n",
    "            action, log_prob = black.get_action(obs[0], env.board.legal_moves, sample_n=sample_n)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "\n",
    "    # return reward based on winning or losing from white/black perspective\n",
    "    if perspective == chess.BLACK and reward == -1:\n",
    "        reward = 1\n",
    "    elif perspective == chess.WHITE and reward == 1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "        \n",
    "    return reward\n",
    "\n",
    "\n",
    "def duel(env, new_model, old_model, num_rounds):\n",
    "    \"\"\" Duel against the previous best model and return the win ratio. \"\"\"\n",
    "    new_model.eval()\n",
    "    old_model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        wins = 0\n",
    "        for i in range(num_rounds):\n",
    "            reward_w = play_game(env, new_model, old_model, perspective=chess.WHITE, sample_n=2)\n",
    "            reward_b = play_game(env, old_model, new_model, perspective=chess.BLACK, sample_n=2)\n",
    "\n",
    "            wins += reward_w + reward_b\n",
    "    new_model.train()    \n",
    "    return wins / (2 * num_rounds)\n",
    "\n",
    "\n",
    "def run_training(num_games=100, duel_every=10, duel_winrate=0.55):\n",
    "    observation, info = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    terminal = False\n",
    "    for g in range(num_games):    \n",
    "        print(f\"Starting game number: {g}\")\n",
    "\n",
    "        g_actions = []\n",
    "        g_states = []\n",
    "\n",
    "        gstep = 0\n",
    "        pbar = tqdm()\n",
    "        while not terminal:\n",
    "            state = env.get_string_representation()\n",
    "\n",
    "            model.eval()\n",
    "            action, value = tree.search(state, observation, simulations_number=1000) # value = ucb\n",
    "            model.train()\n",
    "\n",
    "            if isinstance(value, float):\n",
    "                value = torch.tensor(value, device=DEVICE)\n",
    "\n",
    "            # Gather data\n",
    "            g_actions.append(action)\n",
    "            g_states.append(observation[0])\n",
    "            wandb.log({'UCB': value})\n",
    "\n",
    "            observation, reward, terminal, truncated, info = env.step(action)\n",
    "            \n",
    "            gstep += 1\n",
    "            pbar.update()\n",
    "\n",
    "        # Create training tensors\n",
    "        if reward == 1: # white win\n",
    "            g_values = [(-1)**(i) for i in range(len(g_actions))]\n",
    "        elif reward == -1: # black win\n",
    "            g_values = [(-1)**(i+1) for i in range(len(g_actions))]\n",
    "        else:\n",
    "            g_values = [0] * len(g_actions)\n",
    "\n",
    "        g_values = torch.tensor(g_values, device=DEVICE, dtype=torch.float32)\n",
    "        g_states = torch.tensor(g_states, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "        # Update model with game data\n",
    "        with autocast():   \n",
    "            policy_output, value_output = model(g_states.unsqueeze(1)) \n",
    "            policy_loss = model.policy_loss(policy_output.squeeze(), torch.tensor(g_actions, device=DEVICE))\n",
    "            value_loss = model.value_loss(value_output.squeeze(), g_values)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "        print(f\"Game: {g} - TotalLoss: {loss.item():.6f} - PolicyLoss: {policy_loss.item():.6f} - ValueLoss: {value_loss.item():.6f}\")\n",
    "\n",
    "        # AMP with gradient clipping\n",
    "        model.optimizer.zero_grad()\n",
    "        model.grad_scaler.scale(loss).backward()\n",
    "        model.grad_scaler.unscale_(model.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        model.grad_scaler.step(model.optimizer)\n",
    "        model.grad_scaler.update()\n",
    "\n",
    "        # Duel models and save best \n",
    "        if (gstep % duel_every == 0) and (gstep > 0):\n",
    "            winlose = duel(env, model, best_model, 7)\n",
    "            \n",
    "            wandb.log({\"win/loss:\": winlose})\n",
    "            print(winlose)\n",
    "            if winlose > duel_winrate:\n",
    "                torch.save(model.state_dict(), BESTMODEL_SAVEPATH) \n",
    "            \n",
    "        wandb.log({\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item(), \"total_loss\": loss.item()})\n",
    "        \n",
    "        tree.reset()\n",
    "        observation, info = env.reset()\n",
    "        terminal = False\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkeithg33\u001b[0m (\u001b[33mopen_sim2real\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/wandb/run-20231030_231135-50653cwf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/open_sim2real/Chess/runs/50653cwf' target=\"_blank\">masked-fang-115</a></strong> to <a href='https://wandb.ai/open_sim2real/Chess' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/open_sim2real/Chess' target=\"_blank\">https://wandb.ai/open_sim2real/Chess</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/open_sim2real/Chess/runs/50653cwf' target=\"_blank\">https://wandb.ai/open_sim2real/Chess/runs/50653cwf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting game number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [14:50,  9.96s/it]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mChess\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m run_training(\u001b[39m100\u001b[39;49m, duel_every\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m \u001b[39m# Create training tensors\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m reward \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m: \u001b[39m# white win\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     g_values \u001b[39m=\u001b[39m [(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlen\u001b[39m(g_actions)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39melif\u001b[39;00m reward \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: \u001b[39m# black win\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     g_values \u001b[39m=\u001b[39m [(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlen\u001b[39m(g_actions)]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"Chess\")\n",
    "run_training(100, duel_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
