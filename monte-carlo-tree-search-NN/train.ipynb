{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, pickle\n",
    "from tqdm import tqdm  # notebook compatible\n",
    "import wandb\n",
    "import numpy as np\n",
    "import gym\n",
    "import chess\n",
    "\n",
    "from einops import rearrange, reduce\n",
    "\n",
    "import adversarial_gym\n",
    "from OBM_ChessNetwork import ChessNetworkSimple\n",
    "from search import MonteCarloTreeSearch\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Gym Chess Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Chess-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/.local/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChessNetworkSimple(\n",
       "  (swin_transformer): SwinTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(1, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (layers): Sequential(\n",
       "      (0): SwinTransformerStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.004)\n",
       "            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.009)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.013)\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.017)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.022)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.026)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.030)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.035)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.039)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.043)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.048)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.052)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.057)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.061)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.065)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (12): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.070)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (13): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.074)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (14): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.078)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (15): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.083)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (16): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.087)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (17): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.091)\n",
       "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerStage(\n",
       "        (downsample): PatchMerging(\n",
       "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.096)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SwinTransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): WindowAttention(\n",
       "              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): ClassifierHead(\n",
       "      (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Identity())\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "      (flatten): Identity()\n",
       "    )\n",
       "  )\n",
       "  (policy_head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
       "    (2): Linear(in_features=512, out_features=4672, bias=True)\n",
       "  )\n",
       "  (value_head): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
       "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       "  (value_loss): MSELoss()\n",
       "  (policy_loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH = '/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/best_baseSwinChessNet.pt'\n",
    "BESTMODEL_SAVEPATH = 'mcts_baseSwinChessNet_best.pt'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = ChessNetworkSimple(hidden_dim=512, device=DEVICE)\n",
    "best_model = ChessNetworkSimple(hidden_dim=512, device=DEVICE)\n",
    "\n",
    "if MODEL_PATH is not None:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    best_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MCTS Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = MonteCarloTreeSearch(env, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReplayBuffer:\n",
    "#     \"\"\" Replay buffer to store past experiences for training policy/value network\"\"\"\n",
    "#     def __init__(self, capacity):\n",
    "#         self.actions = np.empty((capacity, 1), dtype=int)\n",
    "#         self.states = np.empty((capacity, 8, 8), dtype=int)\n",
    "#         self.values = np.empty((capacity, 1), dtype=float)\n",
    "\n",
    "#         self.buffer = []\n",
    "#         self.capacity = capacity\n",
    "#         self.position = 0\n",
    "    \n",
    "#     def push(self, state, action, value):    \n",
    "#         if len(self.buffer) < self.capacity:\n",
    "#             self.buffer.append(None)\n",
    "#         self.buffer[self.position] = (state, action, value)\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def update(self, states, actions, values):\n",
    "#         for state, action, value in zip(states, actions, values):\n",
    "#             self.push(state, action, value)\n",
    "\n",
    "#     def sample(num_samples):\n",
    "#         pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, white, black, perspective=None, num_sims=1000):\n",
    "    \"\"\" \n",
    "    Plays a game and returns 1 if chosen perspective has won, else 0.\n",
    "    \n",
    "    Perspective is either Chess.WHITE (1) or Chess.BLACK (0).\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        state = env.get_string_representation()\n",
    "        if step % 2 == 0:\n",
    "            action, ucb = white.search(state, obs, simulations_number=num_sims)\n",
    "        else:\n",
    "            action, ucb = black.search(state, obs, simulations_number=num_sims)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "\n",
    "    # return reward based on winning or losing from white/black perspective\n",
    "    if perspective == chess.BLACK and reward == -1:\n",
    "        reward = 1\n",
    "    elif perspective == chess.WHITE and reward == 1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def play_game(env, white, black, perspective: int = None, sample_n: int =1):\n",
    "    \"\"\" \n",
    "    Play a game and returns whether white or black white. \n",
    "    \n",
    "    Perspective - Chess.WHITE (1) or Chess.BLACK (0).\n",
    "    sample_n - Set number of top moves to sample from\n",
    "\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        if step % 2 == 0:\n",
    "            action, log_prob = white.get_action(obs[0], env.board.legal_moves, sample_n=sample_n)\n",
    "        else:\n",
    "            action, log_prob = black.get_action(obs[0], env.board.legal_moves, sample_n=sample_n)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "\n",
    "    # return reward based on winning or losing from white/black perspective\n",
    "    if perspective == chess.BLACK and reward == -1:\n",
    "        reward = 1\n",
    "    elif perspective == chess.WHITE and reward == 1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "        \n",
    "    return reward\n",
    "\n",
    "\n",
    "def duel(env, old_model, new_model, num_rounds):\n",
    "    \"\"\" Duel against the previous best model and return the win ratio. \"\"\"\n",
    "    new_model.eval()\n",
    "    old_model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        wins = 0\n",
    "        for i in range(num_rounds):\n",
    "            reward_w = play_game(env, new_model, old_model, perspective=chess.WHITE, sample_n=2)\n",
    "            reward_b = play_game(env, old_model, new_model, perspective=chess.BLACK, sample_n=2)\n",
    "\n",
    "            wins += reward_w + reward_b\n",
    "    new_model.train()    \n",
    "    return wins / (2 * num_rounds)\n",
    "\n",
    "\n",
    "def run_training(num_games=100, duel_every=10, duel_winrate=0.55):\n",
    "    observation, info = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    terminal = False\n",
    "    for g in range(num_games):    \n",
    "        print(f\"Starting game number: {g}\")\n",
    "\n",
    "        g_actions = []\n",
    "        g_states = []\n",
    "\n",
    "        gstep = 0\n",
    "        pbar = tqdm()\n",
    "        while not terminal:\n",
    "            state = env.get_string_representation()\n",
    "\n",
    "            model.eval()\n",
    "            action, value = tree.search(state, observation, simulations_number=1000) # value = ucb\n",
    "            model.train()\n",
    "\n",
    "            if isinstance(value, float):\n",
    "                value = torch.tensor(value, device=DEVICE)\n",
    "\n",
    "            # Gather data\n",
    "            g_actions.append(action)\n",
    "            g_states.append(observation[0])\n",
    "            wandb.log({'UCB': value})\n",
    "\n",
    "            observation, reward, terminal, truncated, info = env.step(action)\n",
    "            \n",
    "            gstep += 1\n",
    "            pbar.update()\n",
    "\n",
    "        # Create training tensors\n",
    "        if reward == 1: # white win\n",
    "            g_values = [(-1)**(i) for i in len(g_actions)]\n",
    "        elif reward == -1: # black win\n",
    "            g_values = [(-1)**(i+1) for i in len(g_actions)]\n",
    "        else:\n",
    "            g_values = [0] * len(g_actions)\n",
    "\n",
    "        g_values = torch.tensor(g_values, device=DEVICE, dtype=torch.float32)\n",
    "        g_states = torch.tensor(g_states, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "        # Update model with game data\n",
    "        with autocast():   \n",
    "            policy_output, value_output = model(g_states.unsqueeze(1)) \n",
    "            policy_loss = model.policy_loss(policy_output.squeeze(), torch.tensor(g_actions, device=DEVICE))\n",
    "            value_loss = model.value_loss(value_output.squeeze(), g_values)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "        print(f\"Game: {g} - TotalLoss: {loss.item():.6f} - PolicyLoss: {policy_loss.item():.6f} - ValueLoss: {value_loss.item():.6f}\")\n",
    "\n",
    "        # AMP with gradient clipping\n",
    "        model.optimizer.zero_grad()\n",
    "        model.grad_scaler.scale(loss).backward()\n",
    "        model.grad_scaler.unscale_(model.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        model.grad_scaler.step(model.optimizer)\n",
    "        model.grad_scaler.update()\n",
    "\n",
    "        # Duel models and save best \n",
    "        if (gstep % duel_every == 0) and (gstep > 0):\n",
    "            winlose = duel(env, model, best_model, 7)\n",
    "            \n",
    "            wandb.log({\"win/loss:\": winlose})\n",
    "            print(winlose)\n",
    "            if winlose > duel_winrate:\n",
    "                torch.save(model.state_dict(), BESTMODEL_SAVEPATH) \n",
    "            \n",
    "        wandb.log({\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item(), \"total_loss\": loss.item()})\n",
    "        \n",
    "        tree.reset()\n",
    "        observation, info = env.reset()\n",
    "        terminal = False\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting game number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:34, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BREAK\n",
      "Game: 0 - TotalLoss: 5.346773 - PolicyLoss: 5.291667 - ValueLoss: 0.055107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93657/542587061.py:126: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  g_states = torch.tensor(g_states, device=DEVICE, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Starting game number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:15, 37.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BREAK\n",
      "Game: 1 - TotalLoss: 5.486919 - PolicyLoss: 5.473958 - ValueLoss: 0.012960\n",
      "0.0\n",
      "Starting game number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:24, 42.43s/it]\n",
      "2it [00:38, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BREAK\n",
      "Game: 2 - TotalLoss: 5.342032 - PolicyLoss: 5.334635 - ValueLoss: 0.007397\n",
      "0.0\n",
      "Starting game number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:18, 39.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BREAK\n",
      "Game: 3 - TotalLoss: 5.341198 - PolicyLoss: 5.182292 - ValueLoss: 0.158907\n",
      "0.0\n",
      "Starting game number: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:18, 39.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# wandb.init(project=\"Chess\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m run_training(\u001b[39m10\u001b[39;49m, duel_every\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb Cell 12\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mget_string_representation()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m action, value \u001b[39m=\u001b[39m tree\u001b[39m.\u001b[39;49msearch(state, observation, simulations_number\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m) \u001b[39m# value = ucb\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train.ipynb#X14sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mfloat\u001b[39m):\n",
      "File \u001b[0;32m~/chess_workspace/chess-rl/monte-carlo-tree-search-NN/search.py:45\u001b[0m, in \u001b[0;36mMonteCarloTreeSearch.search\u001b[0;34m(self, init_state, init_obs, simulations_number)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[39m# length = 20\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[39m# traversal_queue = deque(np.array([np.zeros(64) for i in range(length)]), maxlen=length)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     traversal_queue \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_iteration(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame_state, game_obs\u001b[39m=\u001b[39;49minit_obs, traversal_queue\u001b[39m=\u001b[39;49mtraversal_queue)\n\u001b[1;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame_state\u001b[39m.\u001b[39mset_string_representation(init_state)\n\u001b[1;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_action(init_state, c_param\u001b[39m=\u001b[39m\u001b[39m0.\u001b[39m)\n",
      "File \u001b[0;32m~/chess_workspace/chess-rl/monte-carlo-tree-search-NN/search.py:92\u001b[0m, in \u001b[0;36mMonteCarloTreeSearch.search_iteration\u001b[0;34m(self, game_state, game_obs, traversal_queue, c_param)\u001b[0m\n\u001b[1;32m     90\u001b[0m game_state\u001b[39m.\u001b[39mskip_next_human_render()\n\u001b[1;32m     91\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m game_state\u001b[39m.\u001b[39mstep(best_action)\n\u001b[0;32m---> 92\u001b[0m player_at_leaf, predicted_outcome \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_iteration(\n\u001b[1;32m     93\u001b[0m     game_state\u001b[39m=\u001b[39;49mgame_state,\n\u001b[1;32m     94\u001b[0m     game_obs\u001b[39m=\u001b[39;49mobservation,\n\u001b[1;32m     95\u001b[0m     traversal_queue\u001b[39m=\u001b[39;49mtraversal_queue)\n\u001b[1;32m     97\u001b[0m \u001b[39m# result is -1 if previous player won, and 1 if current player won.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m result \u001b[39m=\u001b[39m predicted_outcome \u001b[39mif\u001b[39;00m player_at_leaf \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_player[state] \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m \\\n\u001b[1;32m     99\u001b[0m     predicted_outcome \u001b[39mif\u001b[39;00m player_at_leaf \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_previous_player[state] \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/chess_workspace/chess-rl/monte-carlo-tree-search-NN/search.py:92\u001b[0m, in \u001b[0;36mMonteCarloTreeSearch.search_iteration\u001b[0;34m(self, game_state, game_obs, traversal_queue, c_param)\u001b[0m\n\u001b[1;32m     90\u001b[0m game_state\u001b[39m.\u001b[39mskip_next_human_render()\n\u001b[1;32m     91\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m game_state\u001b[39m.\u001b[39mstep(best_action)\n\u001b[0;32m---> 92\u001b[0m player_at_leaf, predicted_outcome \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_iteration(\n\u001b[1;32m     93\u001b[0m     game_state\u001b[39m=\u001b[39;49mgame_state,\n\u001b[1;32m     94\u001b[0m     game_obs\u001b[39m=\u001b[39;49mobservation,\n\u001b[1;32m     95\u001b[0m     traversal_queue\u001b[39m=\u001b[39;49mtraversal_queue)\n\u001b[1;32m     97\u001b[0m \u001b[39m# result is -1 if previous player won, and 1 if current player won.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m result \u001b[39m=\u001b[39m predicted_outcome \u001b[39mif\u001b[39;00m player_at_leaf \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_player[state] \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m \\\n\u001b[1;32m     99\u001b[0m     predicted_outcome \u001b[39mif\u001b[39;00m player_at_leaf \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_previous_player[state] \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/chess_workspace/chess-rl/monte-carlo-tree-search-NN/search.py:79\u001b[0m, in \u001b[0;36mMonteCarloTreeSearch.search_iteration\u001b[0;34m(self, game_state, game_obs, traversal_queue, c_param)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39m# 1 current state wins, -1 previous state wins\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m# predicted_outcome = np.random.rand()\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m# _, predicted_outcome = model(game_obs[0])\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 79\u001b[0m     _, predicted_outcome \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnnet(game_obs[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     80\u001b[0m \u001b[39m# predicted_outcome = self.nnet.predict(np.array(traversal_queue, dtype=np.uint8).reshape((20, 1, 64)))\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m# predicted_outcome = self.nnet.predict(np.array(traversal_queue, dtype=np.uint8)[np.newaxis, ...])[-1]\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[39m# print('current player:', player_at_leaf, 'player at start:', game_state.starting_player, 'predicted outcome:', predicted_outcome)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m# predicted_outcome = predicted_outcome  * 2 - 1\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m player_at_leaf, predicted_outcome\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/chess_workspace/chess-rl/monte-carlo-tree-search-NN/OBM_ChessNetwork.py:121\u001b[0m, in \u001b[0;36mChessNetworkSimple.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    119\u001b[0m     \u001b[39m# Assuming its 8x8 array. Convert to (1,1,8,8) tensor\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(x, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 121\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mswin_transformer\u001b[39m.\u001b[39;49mforward_features(x)\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m action_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_head(features)\n\u001b[1;32m    123\u001b[0m board_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_head(features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/swin_transformer.py:586\u001b[0m, in \u001b[0;36mSwinTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_features\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    585\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embed(x)\n\u001b[0;32m--> 586\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n\u001b[1;32m    587\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[1;32m    588\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/swin_transformer.py:419\u001b[0m, in \u001b[0;36mSwinTransformerStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 419\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample(x)\n\u001b[1;32m    421\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrad_checkpointing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[1;32m    422\u001b[0m         x \u001b[39m=\u001b[39m checkpoint_seq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/timm/models/swin_transformer.py:341\u001b[0m, in \u001b[0;36mPatchMerging.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    339\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(B, H \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, W \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, C)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m)\u001b[39m.\u001b[39mflatten(\u001b[39m3\u001b[39m)\n\u001b[1;32m    340\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n\u001b[0;32m--> 341\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction(x)\n\u001b[1;32m    342\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wandb.init(project=\"Chess\")\n",
    "run_training(100, duel_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
