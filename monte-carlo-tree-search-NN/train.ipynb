{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, random\n",
    "from tqdm.auto import tqdm \n",
    "import wandb\n",
    "import numpy as np\n",
    "import gym\n",
    "import chess\n",
    "\n",
    "import adversarial_gym\n",
    "from OBM_ChessNetwork import ChessNetworkSimple\n",
    "from search import MonteCarloTreeSearch\n",
    "\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Gym Chess Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Chess-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/best_baseSwinChessNet.pt'\n",
    "BESTMODEL_SAVEPATH = 'mcts_baseSwinChessNet_best.pt'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = ChessNetworkSimple(hidden_dim=512, device=DEVICE)\n",
    "best_model = ChessNetworkSimple(hidden_dim=512, device=DEVICE)\n",
    "\n",
    "if MODEL_PATH is not None:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    best_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MCTS Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = MonteCarloTreeSearch(env, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\" Replay buffer to store past experiences for training policy/value network\"\"\"\n",
    "    def __init__(self, capacity, batch_size):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.values = []\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.curr_length = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, value):    \n",
    "        if len(self.actions) < self.capacity:\n",
    "            self.states.append(None)\n",
    "            self.actions.append(None)\n",
    "            self.values.append(None)\n",
    "        \n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.values[self.position] = value\n",
    "\n",
    "        self.curr_length = len(self.states)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def update(self, states, actions, winner):\n",
    "        # Create value targets based on who won\n",
    "        if winner == 1:\n",
    "            values = [(-1)**(i) for i in range(len(actions))]\n",
    "        elif winner == -1:\n",
    "            values = [(-1)**(i+1) for i in range(len(actions))]\n",
    "        else:\n",
    "            values = [0] * len(actions)\n",
    "\n",
    "        for state, action, value in zip(states, actions, values):\n",
    "            self.push(state, action, value)\n",
    "\n",
    "    def sample(self, ):\n",
    "        indices = random.sample(range(len(self.states)), self.batch_size)\n",
    "        states, actions, values = zip(*[(self.states[i], self.actions[i], self.values[i]) for i in indices])\n",
    "        return states, actions, values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, white, black, perspective=None, num_sims=1000):\n",
    "    \"\"\" \n",
    "    Plays a game and returns 1 if chosen perspective has won, else 0.\n",
    "    \n",
    "    Perspective is either Chess.WHITE (1) or Chess.BLACK (0).\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        state = env.get_string_representation()\n",
    "        if step % 2 == 0:\n",
    "            action, ucb = white.search(state, obs, simulations_number=num_sims)\n",
    "        else:\n",
    "            action, ucb = black.search(state, obs, simulations_number=num_sims)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "\n",
    "    # return reward based on winning or losing from white/black perspective\n",
    "    if perspective == chess.BLACK and reward == -1:\n",
    "        reward = 1\n",
    "    elif perspective == chess.WHITE and reward == 1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "def play_game(env, white, black, perspective: int = None, sample_n: int =1):\n",
    "    \"\"\" \n",
    "    Play a game and returns whether white or black white. \n",
    "    \n",
    "    Perspective - Chess.WHITE (1) or Chess.BLACK (0).\n",
    "    sample_n - Set number of top moves to sample from\n",
    "\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        if step % 2 == 0:\n",
    "            action, log_prob = white.get_action(obs[0], env.board.legal_moves, sample_n=sample_n)\n",
    "        else:\n",
    "            action, log_prob = black.get_action(obs[0], env.board.legal_moves, sample_n=sample_n)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "\n",
    "    # return reward based on winning or losing from white/black perspective\n",
    "    if perspective == chess.BLACK and reward == -1:\n",
    "        reward = 1\n",
    "    elif perspective == chess.WHITE and reward == 1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "        \n",
    "    return reward\n",
    "\n",
    "\n",
    "def duel(env, new_model, old_model, num_rounds):\n",
    "    \"\"\" Duel against the previous best model and return the win ratio. \"\"\"\n",
    "    new_model.eval()\n",
    "    old_model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        wins = 0\n",
    "        for i in range(num_rounds):\n",
    "            reward_w = play_game(env, new_model, old_model, perspective=chess.WHITE, sample_n=2)\n",
    "            reward_b = play_game(env, old_model, new_model, perspective=chess.BLACK, sample_n=2)\n",
    "\n",
    "            wins += reward_w + reward_b\n",
    "    new_model.train()    \n",
    "    return wins / (2 * num_rounds)\n",
    "\n",
    "\n",
    "def update_model(model, replay_buffer, num_iterations):\n",
    "    \"\"\" Sample from replay buffer and train policy and value head\"\"\"\n",
    "    for i in range(num_iterations):\n",
    "        states_batch, actions_batch, values_batch = replay_buffer.sample()\n",
    "        \n",
    "        states_batch = torch.tensor(states_batch, device=DEVICE, dtype=torch.float32).unsqueeze(1)\n",
    "        actions_batch = torch.tensor(actions_batch, device=DEVICE)\n",
    "        values_batch = torch.tensor(values_batch, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "        # Update model \n",
    "        with autocast():   \n",
    "            policy_output, value_output = model(states_batch) \n",
    "            policy_loss = model.policy_loss(policy_output.squeeze(), actions_batch)\n",
    "            value_loss = model.value_loss(value_output.squeeze(), values_batch)\n",
    "            loss = policy_loss + value_loss\n",
    "        \n",
    "        # AMP with gradient clipping\n",
    "        model.optimizer.zero_grad()\n",
    "        model.grad_scaler.scale(loss).backward()\n",
    "        model.grad_scaler.unscale_(model.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        model.grad_scaler.step(model.optimizer)\n",
    "        model.grad_scaler.update()\n",
    "    \n",
    "    return loss.item(), policy_loss.item(), value_loss.item()\n",
    "\n",
    "\n",
    "def run_training(num_games=100, duel_every=10, duel_winrate=0.55, buffer_capacity=1_000_000, buffer_batch_size=64,\n",
    "                 buffer_fillup_period=5):\n",
    "    observation, info = env.reset()\n",
    "    env.render()\n",
    "\n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_capacity, batch_size=buffer_batch_size)\n",
    "\n",
    "    terminal = False\n",
    "    for g in range(num_games):    \n",
    "        print(f\"Starting game number: {g}\")\n",
    "\n",
    "        g_actions = []\n",
    "        g_states = []\n",
    "\n",
    "        gstep = 0\n",
    "        pbar = tqdm()\n",
    "        while not terminal:\n",
    "            state = env.get_string_representation()\n",
    "\n",
    "            model.eval()\n",
    "            action, value = tree.search(state, observation, simulations_number=5000) # value = ucb\n",
    "            model.train()\n",
    "\n",
    "            if isinstance(value, float):\n",
    "                value = torch.tensor(value, device=DEVICE)\n",
    "\n",
    "            # Gather data\n",
    "            g_actions.append(action)\n",
    "            g_states.append(observation[0])\n",
    "            wandb.log({'UCB': value})\n",
    "\n",
    "            observation, reward, terminal, truncated, info = env.step(action)\n",
    "            \n",
    "            gstep += 1\n",
    "            pbar.update()\n",
    "\n",
    "        replay_buffer.update(g_states, g_actions, reward)\n",
    "\n",
    "        if g > buffer_fillup_period: \n",
    "            loss, policy_loss, value_loss = update_model(model, replay_buffer, 10)\n",
    "\n",
    "            print(f\"Game: {g} - TotalLoss: {loss:.6f} - PolicyLoss: {policy_loss:.6f} - ValueLoss: {value_loss:.6f}\")\n",
    "\n",
    "        # Duel models and save best \n",
    "        if (g % duel_every == 0) and (g > 0):\n",
    "            winlose = duel(env, model, best_model, 7)\n",
    "            \n",
    "            wandb.log({\"win/loss:\": winlose})\n",
    "            print(winlose)\n",
    "            if winlose > duel_winrate:\n",
    "                torch.save(model.state_dict(), BESTMODEL_SAVEPATH) \n",
    "            \n",
    "        wandb.log({\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item(), \"total_loss\": loss.item()})\n",
    "        \n",
    "        tree.reset()\n",
    "        observation, info = env.reset()\n",
    "        terminal = False\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=\"Chess\")\n",
    "run_training(100, duel_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
