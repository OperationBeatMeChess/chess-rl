{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS Training Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, pickle, random\n",
    "from tqdm import tqdm \n",
    "import wandb\n",
    "import numpy as np\n",
    "import gym\n",
    "import chess\n",
    "from torch.multiprocessing import Pool, set_start_method, Lock, Process\n",
    "\n",
    "import adversarial_gym\n",
    "from OBM_ChessNetwork import ChessNetworkSimple\n",
    "from search import MonteCarloTreeSearch\n",
    "from parallel import run_games_continuously, torch_safesave, ReplayBufferManager, ReplayBuffer, ChessReplayDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../chess_utils')\n",
    "from chess_dataset import ChessDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:135: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): ChessNetworkSimple(\n",
       "    (swin_transformer): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(1, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers): Sequential(\n",
       "        (0): SwinTransformerStage(\n",
       "          (downsample): Identity()\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): Identity()\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.004)\n",
       "              (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.009)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.013)\n",
       "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.017)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.022)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.026)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.030)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.035)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.039)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (6): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.043)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (7): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.048)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (8): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.052)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (9): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.057)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (10): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.061)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (11): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.065)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (12): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.070)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (13): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.074)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (14): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.078)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (15): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.083)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (16): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.087)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (17): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.091)\n",
       "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerStage(\n",
       "          (downsample): PatchMerging(\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "          )\n",
       "          (blocks): Sequential(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.096)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path1): DropPath(drop_prob=0.100)\n",
       "              (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (drop_path2): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (policy_head): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
       "      (2): Linear(in_features=512, out_features=4672, bias=True)\n",
       "    )\n",
       "    (value_head): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): RReLU(lower=0.125, upper=0.3333333333333333)\n",
       "      (2): Linear(in_features=512, out_features=1, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_loss): MSELoss()\n",
       "    (policy_loss): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_PATH = 'best_baseSwinChessNet.pt'\n",
    "MODEL_PATH = None\n",
    "BESTMODEL_SAVEPATH = 'mcts_baseSwinChessNet_best.pt'\n",
    "CURRMODEL_SAVEPATH = 'currentMCTS' #+ MODEL_PATH\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = ChessNetworkSimple(hidden_dim=512, device=DEVICE)\n",
    "best_model = ChessNetworkSimple(hidden_dim=512, device=DEVICE)\n",
    "\n",
    "if MODEL_PATH is not None:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    best_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "model = torch.compile(model)\n",
    "x = torch.randn((1,1,8,8), device='cuda', dtype=torch.float32)\n",
    "out = model(x)\n",
    "best_model = torch.compile(best_model)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReplayBuffer:\n",
    "#     \"\"\" Replay buffer to store past experiences for training policy/value network\"\"\"\n",
    "#     def __init__(self, capacity=None):\n",
    "#         self.actions = []\n",
    "#         self.states = []\n",
    "#         self.values = []\n",
    "\n",
    "#         self.capacity = capacity\n",
    "#         self.curr_length = 0\n",
    "#         self.position = 0\n",
    "    \n",
    "#     def get_state(self):\n",
    "#         return {\n",
    "#             'actions': list(self.actions),\n",
    "#             'states': list(self.states),\n",
    "#             'values': list(self.values),\n",
    "#             'capacity': self.capacity,\n",
    "#             'curr_length': self.curr_length,\n",
    "#             'position': self.position\n",
    "#         }\n",
    "\n",
    "#     def from_dict(self, buffer_state_dict):\n",
    "#         for key, value in buffer_state_dict.items():\n",
    "#             setattr(self, key, value)\n",
    "    \n",
    "#     def push(self, state, action, value):    \n",
    "#         if len(self.actions) < self.capacity:\n",
    "#             self.states.append(None)\n",
    "#             self.actions.append(None)\n",
    "#             self.values.append(None)\n",
    "        \n",
    "#         self.states[self.position] = state\n",
    "#         self.actions[self.position] = action\n",
    "#         self.values[self.position] = value\n",
    "\n",
    "#         self.curr_length = len(self.states)\n",
    "#         self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "#     def update(self, states, actions, winner):\n",
    "#         # Create value targets based on who won\n",
    "#         if winner == 1:\n",
    "#             values = [(-1)**(i) for i in range(len(actions))]\n",
    "#         elif winner == -1:\n",
    "#             values = [(-1)**(i+1) for i in range(len(actions))]\n",
    "#         else:\n",
    "#             values = [0] * len(actions)\n",
    "\n",
    "#         for state, action, value in zip(states, actions, values):\n",
    "#             self.push(state, action, value)\n",
    "\n",
    "# class ChessReplayDataset(Dataset):\n",
    "#     def __init__(self, replay_buffer_proxy):\n",
    "#         # Initialize the dataset with replay buffer data\n",
    "#         self.replay_buffer = ReplayBuffer().from_dict(replay_buffer_proxy.get_state())\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # Return the current size of the replay buffer\n",
    "#         return self.replay_buffer.curr_length\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Fetch a single experience at the specified index\n",
    "#         if idx >= len(self):\n",
    "#             raise IndexError('Index out of range in ChessReplayDataset')\n",
    "#         state = self.replay_buffer.states[idx]\n",
    "#         action = self.replay_buffer.actions[idx]\n",
    "#         value = self.replay_buffer.values[idx]\n",
    "#         return state, action, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, white, black, perspective: int = None, sample_n: int = 1):\n",
    "    \"\"\" \n",
    "    Play a game and returns whether white or black white. \n",
    "    \n",
    "    Perspective - Chess.WHITE (1) or Chess.BLACK (0).\n",
    "    sample_n - Set number of top moves to sample from\n",
    "\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    done = False\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        if step % 2 == 0:\n",
    "            action, log_prob = white.get_action(obs[0], env.board.legal_moves, sample_n=sample_n)\n",
    "        else:\n",
    "            action, log_prob = black.get_action(obs[0], env.board.legal_moves, sample_n=sample_n)\n",
    "\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        step += 1\n",
    "\n",
    "    # return reward based on winning or losing from white/black perspective\n",
    "    if perspective == chess.BLACK and reward == -1:\n",
    "        reward = 1\n",
    "    elif perspective == chess.WHITE and reward == 1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 0\n",
    "        \n",
    "    return reward\n",
    "\n",
    "\n",
    "def duel(env, new_model, old_model, num_rounds):\n",
    "    \"\"\" Duel against the previous best model and return the win ratio. \"\"\"\n",
    "    new_model.eval()\n",
    "    old_model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        wins = 0\n",
    "        for i in range(num_rounds):\n",
    "            reward_w = play_game(env, new_model, old_model, perspective=chess.WHITE, sample_n=2)\n",
    "            reward_b = play_game(env, old_model, new_model, perspective=chess.BLACK, sample_n=2)\n",
    "\n",
    "            wins += reward_w + reward_b\n",
    "    new_model.train()    \n",
    "    return wins / (2 * num_rounds)\n",
    "\n",
    "\n",
    "def update_model(model, selfplay_buffer_proxy, expert_dataset, dataset_size):\n",
    "    \"\"\" \n",
    "    Train on selfplay and expert data. Builds a dataset of size dataset_size, where the\n",
    "    proportion of data comes from,\n",
    "    \n",
    "        expert_size + selfplay_size = dataset_size \n",
    "    \n",
    "    If the replay buffer has more data than dataset_size, will sample from selfplay data only\n",
    "    \n",
    "    \"\"\"\n",
    "    # Initialize buffer dataset\n",
    "    print(\"updating model\")\n",
    "    selfplay_dataset = ChessReplayDataset(selfplay_buffer_proxy)\n",
    "    \n",
    "    expert_size = dataset_size - len(selfplay_dataset)\n",
    "\n",
    "    if expert_size > 0: # combine data\n",
    "        indices = random.sample(range(1, len(expert_dataset)), expert_size)\n",
    "        expert_subset = torch.utils.data.Subset(expert_dataset, indices)\n",
    "        train_dataset = ConcatDataset([expert_subset, selfplay_dataset])\n",
    "\n",
    "    else: # selfplay data\n",
    "        indices = random.sample(range(1, len(selfplay_dataset)), dataset_size)\n",
    "        train_dataset = torch.utils.data.Subset(selfplay_dataset, indices)\n",
    "\n",
    "    # Create dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    print(\"dataset and trainloader created\")\n",
    "    # Initialize losses\n",
    "    total_policy_loss = 0\n",
    "    total_value_loss = 0\n",
    "    total_loss = 0\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for (states_batch, actions_batch, values_batch) in train_loader:\n",
    "        print(\"doing batch\")\n",
    "        states_batch = states_batch.to(model.device, dtype=torch.float32).unsqueeze(1)\n",
    "        actions_batch = actions_batch.to(model.device, dtype=torch.long)\n",
    "        values_batch = values_batch.to(model.device, dtype=torch.float32)\n",
    "\n",
    "        # Forward pass and calculate loss\n",
    "        with autocast():\n",
    "            policy_output, value_output = model(states_batch)\n",
    "            policy_loss = model.policy_loss(policy_output.squeeze(), actions_batch)\n",
    "            value_loss = model.value_loss(value_output.squeeze(), values_batch)\n",
    "            loss = policy_loss + value_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        model.optimizer.zero_grad()\n",
    "        model.grad_scaler.scale(loss).backward()\n",
    "        model.grad_scaler.unscale_(model.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        model.grad_scaler.step(model.optimizer)\n",
    "        model.grad_scaler.update()\n",
    "\n",
    "        # Record the losses\n",
    "        total_policy_loss += policy_loss.item()\n",
    "        total_value_loss += value_loss.item()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Average the losses over the iterations\n",
    "    avg_policy_loss = total_policy_loss / len(train_loader)\n",
    "    avg_value_loss = total_value_loss / len(train_loader)\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    return avg_loss, avg_policy_loss, avg_value_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_bufferproxy(buffer_proxy):\n",
    "    shared_buffer_state = buffer_proxy.get_state()\n",
    "    with open('replay_buffer_state.pkl', 'wb') as f:\n",
    "        pickle.dump(shared_buffer_state, f)\n",
    "        \n",
    "def run_training(num_games, expert_dataset, games_in_parallel, train_every, duel_every=10, duel_winrate=0.55, buffer_capacity=1_000_000):\n",
    "    # Multiprocessing stuff\n",
    "    manager = ReplayBufferManager()\n",
    "    manager.start()\n",
    "    shared_replay_buffer = manager.ReplayBuffer(capacity=buffer_capacity)\n",
    "    shutdown_event = manager.Event()\n",
    "    buffer_lock = manager.Lock()\n",
    "    global_game_counter = manager.GameCounter()  # Initialize a shared counter\n",
    "    file_lock = Lock()\n",
    "    \n",
    "    model_state = model.state_dict()\n",
    "    model_state = {k: v.cpu() for k, v in model.state_dict().items()} # can't share cuda tensors\n",
    "\n",
    "    # Start the continuous game running process in a separate process\n",
    "    process = Process(target=run_games_continuously, args=(model_state, CURRMODEL_SAVEPATH, shared_replay_buffer, games_in_parallel, buffer_lock, file_lock, global_game_counter, shutdown_event))\n",
    "    process.start()\n",
    "\n",
    "    train_flag = False\n",
    "    duel_flag = False\n",
    "\n",
    "    # Main training loop\n",
    "    env = gym.make(\"Chess-v0\")\n",
    "    training = True\n",
    "    while training:\n",
    "        game_count = global_game_counter.count\n",
    "        if game_count > 0 and game_count % train_every == 0:\n",
    "            train_flag = True\n",
    "        if game_count > 0 and game_count % duel_every == 0:\n",
    "            duel_flag = True\n",
    "        if game_count >= num_games:\n",
    "            shutdown_event.set()\n",
    "            training = False # don't break so we can train and duel one last time\n",
    "            \n",
    "            # train_flag = True\n",
    "            # duel_flag = True\n",
    "\n",
    "        if train_flag:\n",
    "            loss, policy_loss, value_loss = update_model(model, shared_replay_buffer, expert_dataset, 100)\n",
    "            print(loss, policy_loss, value_loss)\n",
    "            # wandb.log({\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item(), \"total_loss\": loss.item()})\n",
    "            torch_safesave(model.state_dict(), CURRMODEL_SAVEPATH, file_lock)\n",
    "            pickle_bufferproxy(shared_replay_buffer)\n",
    "            train_flag = False\n",
    "        \n",
    "        if duel_flag:\n",
    "            winlose = duel(env, model, best_model, 7)\n",
    "            # wandb.log({\"win/loss:\": winlose})\n",
    "            print(winlose)\n",
    "            if winlose > duel_winrate: \n",
    "                torch.save(model.state_dict(), BESTMODEL_SAVEPATH)\n",
    "            duel_flag = False\n",
    "\n",
    "    process.join()\n",
    "\n",
    "    # Done. Save model and buffer\n",
    "    torch.save(model.state_dict(), CURRMODEL_SAVEPATH)\n",
    "    shared_buffer_state = shared_replay_buffer.get_state()\n",
    "    with open('replay_buffer.pkl', 'wb') as f:\n",
    "        pickle.dump(shared_buffer_state, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322\n"
     ]
    }
   ],
   "source": [
    "PGN_FILE = \"/home/kage/chess_workspace/PGN-data/tcec+alphastock/TCEC_Cup_1_Final_5.pgn\"\n",
    "\n",
    "# Load the datasets\n",
    "expert_dataset = ChessDataset(PGN_FILE)\n",
    "print(len(expert_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameoverNOW TRAINING\n",
      "updating model\n",
      "\n",
      "dataset and trainloader created\n",
      "doing batch\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/kage/chess_workspace/chess_venv311/lib/python3.11/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "doing batch\n",
      "8.83162260055542 8.57621955871582 0.2554032653570175\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pickle_safesave() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m NUM_GAMES \u001b[39m=\u001b[39m \u001b[39m25\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m GAMES_IN_PARALLEL \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m run_training(NUM_GAMES, expert_dataset, GAMES_IN_PARALLEL, train_every\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, duel_every\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb#X21sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# wandb.log({\"policy_loss\": policy_loss.item(), \"value_loss\": value_loss.item(), \"total_loss\": loss.item()})\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb#X21sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     torch_safesave(model\u001b[39m.\u001b[39mstate_dict(), CURRMODEL_SAVEPATH, file_lock)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb#X21sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     pickle_safesave(shared_replay_buffer, buffer_lock)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb#X21sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     train_flag \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kage/chess_workspace/chess-rl/monte-carlo-tree-search-NN/train_parallel.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mif\u001b[39;00m duel_flag:\n",
      "\u001b[0;31mTypeError\u001b[0m: pickle_safesave() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gameover\n",
      "gameover\n",
      "gameover\n",
      "gameover\n"
     ]
    }
   ],
   "source": [
    "# wandb.init(project=\"Chess\")\n",
    "set_start_method('spawn', force=True)\n",
    "\n",
    "NUM_GAMES = 25\n",
    "GAMES_IN_PARALLEL = 4\n",
    "\n",
    "\n",
    "run_training(NUM_GAMES, expert_dataset, GAMES_IN_PARALLEL, train_every=5, duel_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
