{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chess RL + Expert Learning + Weirdness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import chess\n",
    "import gym\n",
    "import chess\n",
    "import os, sys, copy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "import adversarial_gym\n",
    "from adversarial_gym.chess_env import ChessEnv\n",
    "\n",
    "from OBM_ChessNetwork import Chess42069NetworkSimple\n",
    "\n",
    "sys.path.append('../../chess_utils')\n",
    "from chess_dataset import ChessDataset\n",
    "from utils import RunningAverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Gameplay Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, white, black, perspective=None, sample_n=1, duel=False):\n",
    "    step = 0\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    observations = []\n",
    "    values = []\n",
    "    done_mask = []\n",
    "    done = False\n",
    "    obs = env.reset()[0]\n",
    "    while not done:\n",
    "        # Note: Modify the get_action function to return the action, log_prob, and state value\n",
    "        if step % 2 == 0:\n",
    "            action_logits, value_estimate = white(obs[0])\n",
    "            action, log_prob = white.to_action(action_logits, env.board.legal_moves, sample_n) # same for black/white\n",
    "        else:\n",
    "            action_logits, value_estimate = black(obs[0])\n",
    "            action, log_prob = black.to_action(action_logits, env.board.legal_moves, sample_n) # same for black/white\n",
    "\n",
    "        if perspective is None or perspective == chess.WHITE and step % 2 == 0 or perspective == chess.BLACK and step % 2 == 1:\n",
    "            observations.append(obs[0])\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value_estimate)\n",
    "            done_mask.append(0 if not done else 1)\n",
    "\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        step += 1\n",
    "        \n",
    "    if reward in [-1,1]:\n",
    "        print(f\"PERSPECTIVE: {perspective} - GAME OUTCOME: {reward}\")\n",
    "    else:\n",
    "        print(f\"DRAW\")\n",
    "\n",
    "    # Reward is 1 if chosen perspective won and -1 if chosen perspective lost\n",
    "    # If dueling set reward to 1 for a win and zero otherwise\n",
    "    if perspective is not None:\n",
    "        reward = 1 if (reward == -1 and perspective == chess.BLACK) else reward\n",
    "        reward = -1 if (reward == 1 and perspective == chess.BLACK) else reward\n",
    "\n",
    "        if duel and reward != 1: reward = 0\n",
    "\n",
    "    rewards = prepare_game_rewards(reward, perspective, len(actions)) if not duel else reward\n",
    "\n",
    "    return observations, actions, log_probs, values, done_mask, rewards\n",
    "\n",
    "\n",
    "def prepare_game_rewards(reward, perspective, game_len):\n",
    "    if perspective is not None:\n",
    "        # Reward in [-1, 0, 1] \n",
    "        rewards = [reward for _ in range(game_len)]\n",
    "        return rewards\n",
    "    \n",
    "    # Self play alternates +/- reward\n",
    "    # Reward in [-1,0,1].\n",
    "    rewards = [reward if i % 2 == 0 else -reward for i in range(game_len)]\n",
    "\n",
    "    rewards = compute_discounted_rewards(reward)\n",
    "    return rewards\n",
    "    \n",
    "    \n",
    "def compute_discounted_rewards(reward, gamma=0.99):\n",
    "    \"\"\"Compute discounted rewards for a sequence of rewards.\"\"\"\n",
    "    n = len(reward)\n",
    "    discounted_rewards = [0] * n\n",
    "    running_add = 0\n",
    "    for t in reversed(range(n)):\n",
    "        running_add = running_add * gamma + reward[t]\n",
    "        discounted_rewards[t] = running_add\n",
    "    return discounted_rewards\n",
    "\n",
    "\n",
    "def duel(env, old_model, new_model, num_rounds):\n",
    "    \"\"\" Duel against the previous best model and return the win ratio. \"\"\"\n",
    "    new_model.eval()\n",
    "    with torch.no_grad():\n",
    "        wins = 0\n",
    "        for i in range(num_rounds):\n",
    "            _, _, _, _, _, r_w = play_game(env, new_model, old_model, perspective=chess.WHITE, sample_n = 2, duel=True)\n",
    "            _, _, _, _, _, r_b = play_game(env, old_model, new_model, perspective=chess.BLACK, sample_n = 2, duel=True)\n",
    "\n",
    "            wins += r_w + r_b\n",
    "    new_model.train()    \n",
    "    return wins / (2 * num_rounds)\n",
    "\n",
    "\n",
    "def self_play(env, model, num_games):\n",
    "    \"\"\" Plays num_games against itself to gather obs, actions, log_probs, rewards data \"\"\"\n",
    "    # TODO: check if numpy array of shape (num_games, 4) is faster, each row could be output of play_game\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    observations = []\n",
    "    for _ in range(num_games):\n",
    "        g_obs, g_actions, g_log_probs, g_reward = play_game(env, model, model, perspective=None)\n",
    "        actions.append(g_actions)\n",
    "        log_probs.append(g_log_probs)\n",
    "        rewards.append(g_reward)\n",
    "        observations.append(g_obs)\n",
    "    return observations, actions, log_probs, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expert Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, val_loader, stats):\n",
    "    model.eval()\n",
    "    stats.reset(\"val_loss\")\n",
    "    t1 = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        for i, (state, action, result) in enumerate(val_loader):\n",
    "            state = state.float().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            action = action.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            result = result.float().to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            \n",
    "            policy_output, value_output = model(state.unsqueeze(1))\n",
    "            policy_loss = model.policy_loss(policy_output.squeeze(), action)\n",
    "            value_loss = model.val_loss(value_output.squeeze(), result)\n",
    "            \n",
    "            loss = policy_loss + value_loss\n",
    "            stats.update(\"val_loss\", loss.item())\n",
    "    \n",
    "    print(f\"Mean Validation Loss: {stats.get_average('val_loss')}, time elapsed: {time.perf_counter()-t1} seconds\")\n",
    "    return stats.get_average('val_loss')\n",
    "\n",
    "\n",
    "def expert_study(model, dataset, percent_dataset=0.1):\n",
    "    \"\"\" Trains on TCEC data in a supervised fashion (behaviour cloning)\"\"\"\n",
    "\n",
    "    # Load random subset of dataset and split\n",
    "    study_size = int(percent_dataset * len(dataset))\n",
    "    random_indices = np.random.randint(0, study_size, study_size)\n",
    "    study_dataset = Subset(dataset, random_indices)\n",
    "    \n",
    "    train_ratio = 0.9\n",
    "    train_size = int(train_ratio * study_size)\n",
    "    val_size = study_size - train_size\n",
    "    train_dataset, val_dataset = random_split(study_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create data loaders for the training and validation sets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, \n",
    "                            pin_memory=False, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, \n",
    "                            pin_memory=False, num_workers=2)\n",
    "\n",
    "    stats = RunningAverage()\n",
    "    stats.add([\"train_loss\", \"val_loss\", \"train_p_loss\", \"train_v_loss\"])\n",
    "\n",
    "    model.train()\n",
    "    t1 = time.perf_counter()\n",
    "    for i, (state, action, result) in enumerate(train_loader):\n",
    "        state = state.float().to(model.device)\n",
    "        action = action.to(model.device)\n",
    "        result = result.float().to(model.device)\n",
    "\n",
    "        with autocast():\n",
    "            policy_output, value_output = model(state.unsqueeze(1))\n",
    "            policy_loss = model.policy_loss(policy_output.squeeze(), action)\n",
    "            value_loss = model.val_loss(value_output.squeeze(), result)\n",
    "            loss = policy_loss + value_loss\n",
    "        \n",
    "        # AMP with gradient clipping\n",
    "        model.optimizer.zero_grad()\n",
    "        model.grad_scaler.scale(loss).backward()\n",
    "        model.grad_scaler.unscale_(model.optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        model.grad_scaler.step(model.optimizer)\n",
    "        model.grad_scaler.update()\n",
    "\n",
    "        stats.update({\n",
    "            \"train_loss\": loss.item(),\n",
    "            \"train_p_loss\": policy_loss.item(),\n",
    "            \"train_v_loss\": value_loss.item()\n",
    "            })\n",
    "        \n",
    "    print(f\"Study Train Loss: {stats.get_average('train_loss')}\")\n",
    "    # wandb.log({\"study_train_loss\": stats.get_average('train_loss')})\n",
    "    t2 = time.perf_counter()\n",
    "    valid_loss = run_validation(model, val_loader, stats)\n",
    "    # wandb.log({\"val_loss\": valid_loss, \"iter\": i})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "MODEL_PATH = '/home/kage/chess_workspace/simpler_SwinChessNet42069.pt'\n",
    "\n",
    "model = Chess42069NetworkSimple(hidden_dim=256, device='cuda')\n",
    "best_model = Chess42069NetworkSimple(hidden_dim=256, device='cuda')\n",
    "\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Loading model at: {MODEL_PATH}\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    best_model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train - VPG with Self-Play and Dueling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Params\n",
    "PGN_FILE = '/home/kage/chess_workspace/PGN-data/alphazero_stockfish_all/alphazero_vs_stockfish_all.pgn'\n",
    "MODEL_SAVEPATH = '/home/kage/chess_workspace/WACKY_RL_MODEL.pt'\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "STUDY_EVERY = 1 \n",
    "DUEL_EVERY = 10\n",
    "\n",
    "chess_dataset = ChessDataset(PGN_FILE)\n",
    "env = gym.make(\"Chess-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(NUM_EPOCHS)):\n",
    "    # Play games as white or black against the previous best model\n",
    "    if i % 2 == 0:\n",
    "        observations, actions, log_probs, values, done_mask, rewards = play_game(env, model, best_model, perspective=chess.WHITE, sample_n=3)\n",
    "    else:\n",
    "        observations, actions, log_probs, values, done_mask, rewards = play_game(env, best_model, model, perspective=chess.BLACK, sample_n=3)\n",
    "    \n",
    "    next_values = values[1:] + [0] # Value of next state, 0 for final action\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    # observations = torch.as_tensor(observations, dtype=torch.float32, device=model.device)\n",
    "    actions = torch.as_tensor(actions, dtype=torch.int64, device= model.device)\n",
    "    log_probs = torch.stack(log_probs).to(model.device)\n",
    "    values = torch.as_tensor(values, dtype=torch.float32, device= model.device)\n",
    "    rewards = torch.as_tensor(rewards, dtype=torch.float32, device= model.device)\n",
    "    next_values = torch.as_tensor(next_values, dtype=torch.float32, device= model.device)\n",
    "    done_mask = torch.as_tensor(done_mask, dtype=torch.float32, device= model.device)\n",
    "\n",
    "    model.update_network(log_probs, rewards, values, next_values, done_mask, gamma=0.99)\n",
    "\n",
    "    # # # Expert Study\n",
    "    # # # if i % STUDY_EVERY == 0:\n",
    "    # # if True:\n",
    "    # #     expert_study(model, chess_dataset, percent_dataset=0.05)\n",
    "\n",
    "    # Darwinian duel to the death\n",
    "    if i % DUEL_EVERY == 0:\n",
    "        win_ratio = duel(env, best_model, model, num_rounds=10)\n",
    "        print(f\"Model win ratio: {win_ratio}\")\n",
    "        if win_ratio > 0.6:\n",
    "            print(\"Best model was deafeted!\")\n",
    "            best_model = copy.deepcopy(model)\n",
    "            torch.save(model.state_dict(), MODEL_SAVEPATH)\n",
    "            best_model.eval()\n",
    "\n",
    "    # # Self play\n",
    "    # obss, actions, log_probs, reward = play_game(env, model, model, perspective=None)\n",
    "    # model.update_policy(model, log_probs, rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
